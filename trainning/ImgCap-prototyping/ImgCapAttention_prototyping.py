# -*- coding: utf-8 -*-
"""imgcapattation-prototypinga.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xlZ-Unn8jRg5jmzfPnorYmKCs9434eqM

### Imports
"""

# !pip install -q  torch==2.2.0 opencv-python torchvision==0.17 torchtext==0.17.0 pycocoevalcap onnx torchinfo datasets
# !python -q -m spacy download en_core_web_sm
# !wget https://raw.githubusercontent.com/Sh-31/ImgCap/main/vocab.pkl
# !pip install -q torcheval

"""### DataLoader"""

import torch
import torch.nn as nn
import numpy as np
import torch.onnx as onnx
import torchvision.models as models
import torch.nn.functional as F
import pandas as pd
import pickle
import string
import spacy
import cv2
from torch.utils.data import Dataset , random_split
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
from torchinfo import summary
from datasets import load_dataset
from torchvision.transforms import v2
from torch.nn.utils.rnn import pad_sequence

ds = load_dataset("shredder-31/Flickr30_sample")

ds

class Flickr30(Dataset):
    def __init__(self, dataset, vocab=None, vocab_path='ImgCap/vocab.pkl',transform=None):
        self.dataset = dataset
        self.transform = transform

        # Initialize spaCy model for tokenization
        self.nlp = spacy.load('en_core_web_sm')  # Can use split(' ')

        if vocab is None:
            self.vocab = self.build_vocab()
            self.save_vocab(self.vocab, vocab_path)
        else:
            self.vocab = vocab

    def build_vocab(self):
        '''Tokenize all captions and build vocabulary'''

        captions = self.dataset['train']['comment']
        tokenized_captions = [self.tokenize_caption(caption) for caption in captions]

        # 997 (token) + 26 (English alphabet) + space = 1024 tokens
        vocab = build_vocab_from_iterator(tokenized_captions, specials=["<unk>", "<pad>", "<sos>", "<eos>"], max_tokens=997, min_freq=5)

        # Add individual English alphabet characters and space to the vocabulary
        for ch in string.ascii_lowercase + ' ':
            if ch not in vocab:
                vocab.append_token(ch)

        vocab.set_default_index(vocab["<unk>"])
        return vocab

    def save_vocab(self, vocab, vocab_path):
        with open(vocab_path, 'wb') as f:
            pickle.dump(vocab, f)
        print(f"Vocabulary saved at {vocab_path}")

    def tokenize_caption(self, caption):
        return [f" {token.text.lower()}" for token in self.nlp(caption)]

    def encoder(self, text):
        tokens = self.tokenize_caption(text)
        indices = [self.vocab["<sos>"]]

        for token in tokens:
            if token in self.vocab:
                indices.append(self.vocab[token])
            else:
                # Split the word into characters if the word is not found in the vocabulary
                for ch in token:
                    indices.append(self.vocab[ch])

        indices.append(self.vocab["<eos>"])
        return torch.tensor(indices, dtype=torch.long)

    def decoder(self, indices):
        tokens = [self.vocab.lookup_token(idx) for idx in indices]
        words = []
        current_word = []
        for token in tokens:
            if len(token) == 1 and token in string.ascii_lowercase:
                current_word.append(token)
            else:
                if current_word:
                    words.append("".join(current_word))
                    current_word = []
                words.append(token)

        if current_word:
            words.append(" "+"".join(current_word))

        return "".join(words)

    def __len__(self):
        return len(self.dataset['train'])

    def __getitem__(self, idx):
        sample = self.dataset['train'][idx]
        image = sample['image']
        caption = sample['comment']

        if self.transform:
            image = self.transform(image)

        tensor_caption = self.encoder(caption)

        return image, tensor_caption

transforms = v2.Compose([
    v2.Resize(size=(224, 224)),
    v2.RandomApply([
        v2.RandomHorizontalFlip(),
        v2.RandomRotation(degrees=15),
        v2.RandomCrop(size=(200, 200)),
        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    ], p=0.4),
    v2.Resize(size=(224, 224)),
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

with open("/kaggle/working/vocab.pkl", 'rb') as f:
      vocab = pickle.load(f)

dataset = Flickr30(ds,vocab=vocab, transform=transforms)

def collate_fn(batch):
    images, captions = zip(*batch)
    images = torch.stack(images, 0)
    captions = pad_sequence(captions, batch_first=True, padding_value=1)
    return images, captions

train_size = int(0.80 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)
val_data_loader = DataLoader(val_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)

for image, cap in train_data_loader:
  print(dataset.decoder(cap[7]))
  break

decoder = dataset.decoder
vocab = dataset.vocab

"""### Modeling"""

## ResNet50 (CNN Encoder) with Spatial Features

class ResNet50(nn.Module):
    def __init__(self):
        super(ResNet50, self).__init__()
        self.ResNet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        self.features = nn.Sequential(*list(self.ResNet50.children())[:-2])
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))

        for param in self.ResNet50.parameters():
            param.requires_grad = False

    def forward(self, x):
        x = self.features(x)  # (B, 2048, 7, 7)
        x = self.avgpool(x)   # Ensure 7x7 output
        B, C, H, W = x.size()
        x = x.view(B, C, -1)  # Flatten spatial dimensions: (B, 2048, 49)
        x = x.permute(0, 2, 1)  # (B, 49, 2048) - 49 spatial locations
        return x

class Attention(nn.Module):
    def __init__(self, feature_size, hidden_size):
        super(Attention, self).__init__()
        self.attention = nn.Linear(feature_size + hidden_size, hidden_size)
        self.attn_weights = nn.Linear(hidden_size, 1)

    def forward(self, features, hidden_state): # features: (B, 49, 2048), hidden_state: (B, hidden_size)
        hidden_state = hidden_state.unsqueeze(1).repeat(1, features.size(1), 1)  # (B, 49, hidden_size)
        combined = torch.cat((features, hidden_state), dim=2)  # (B, 49, feature_size + hidden_size)
        attn_hidden = torch.tanh(self.attention(combined))  # (B, 49, hidden_size)
        attention_logits = self.attn_weights(attn_hidden).squeeze(2)  # (B, 49)
        attention_weights = torch.softmax(attention_logits, dim=1)  # (B, 49)
        context = (features * attention_weights.unsqueeze(2)).sum(dim=1)  # (B, 2048)
        return context, attention_weights

# Attention without learnable paramters:

# logits = torch.matmul(features, hidden_state.unsqueeze(2))  # (B, 49, 1) - Batch Matriax
# attention_weights = torch.softmax(logits, dim=1).squeeze(2)  # (B, 49)
# context = (features * attention_weights.unsqueeze(2)).sum(dim=1)  # (B, 2048)

class lstm(nn.Module):
    def __init__(self, feature_size, hidden_size, number_layers, embedding_dim, vocab_size):
        super(lstm, self).__init__()

        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.attention = Attention(feature_size, hidden_size)

        self.lstm = nn.LSTM(
            input_size=hidden_size + feature_size,  # input: concatenated context and word embedding
            hidden_size=hidden_size,
            num_layers=number_layers,
            dropout=0.5,
            batch_first=True,
        )

        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions=None, max_seq_len=None, teacher_forcing_ratio=0.90):

        batch_size = features.size(0)
        max_seq_len = max_seq_len if max_seq_len is not None else captions.size(1)
        h, c = self.init_hidden_state(batch_size)

        outputs = torch.zeros(batch_size, max_seq_len, self.fc.out_features).to(features.device)
        word_input = torch.full((batch_size,), vocab['<sos>'], dtype=torch.long).to(features.device)

        for t in range(1, max_seq_len):
            embeddings = self.embedding(word_input)
            context, _ = self.attention(features, h[-1])
            lstm_input_step = torch.cat([embeddings, context], dim=1).unsqueeze(1)  # Combine context + word embedding

            out, (h, c) = self.lstm(lstm_input_step, (h, c))
            output = self.fc(out.squeeze(1))
            outputs[:, t, :] = output

            top1 = output.argmax(1)

            if captions is not None and torch.rand(1).item() < teacher_forcing_ratio:
                word_input = captions[:, t]  # Use ground truth caption
            else:
                word_input = top1

        return outputs

    def init_hidden_state(self, batch_size):
        device = next(self.parameters()).device
        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_size).to(device)
        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_size).to(device)
        return (h0, c0)

class ImgCap(nn.Module):
    def __init__(self, feature_size, lstm_hidden_size, num_layers, vocab_size, embedding_dim):
        super(ImgCap, self).__init__()
        self.cnn = ResNet50()
        self.lstm = lstm(feature_size, lstm_hidden_size, num_layers, embedding_dim, vocab_size)

    def forward(self, images, captions):
        cnn_features = self.cnn(images)
        output = self.lstm(cnn_features, captions)  # Remove last token (EOS) from input
        return output

    def generate_caption(self, images, vocab, decoder, device="cpu", start_token="<sos>", end_token="<eos>", max_seq_length=100):
        self.eval()

        with torch.no_grad():
            start_index = vocab[start_token]
            end_index = vocab[end_token]
            images = images.to(device)
            batch_size = images.size(0)

            captions = [[start_index,] for _ in range(batch_size)]
            end_token_appear = [False] * batch_size

            cnn_features = self.cnn(images)  # (B, 49, 2048)

            h, c = self.lstm.init_hidden_state(batch_size)
            # Initialize the first input as the <sos> token
            word_input = torch.full((batch_size,), start_index, dtype=torch.long).to(device)

            for t in range(max_seq_length):
                # Pass the CNN features and the current word input through the LSTM
                embeddings = self.lstm.embedding(word_input)
                context, _ = self.lstm.attention(cnn_features, h[-1])  # Attention context
                lstm_input_step = torch.cat([embeddings, context], dim=1).unsqueeze(1)  # Combine context + word embedding

                out, (h, c) = self.lstm.lstm(lstm_input_step, (h, c))

                output = self.lstm.fc(out.squeeze(1))  # (B, vocab_size)

                # Get the predicted word (greedy search)
                predicted_word_indices = torch.argmax(output, dim=1)  # (B,)
                word_input = predicted_word_indices

                # Append the predicted word to the caption, if it's not the <eos> token
                for i in range(batch_size):
                    if not end_token_appear[i]:
                        predicted_word = vocab.lookup_token(predicted_word_indices[i].item())
                        if predicted_word == end_token:
                            captions[i].append(predicted_word_indices[i].item())
                            end_token_appear[i] = True
                        else:
                             captions[i].append(predicted_word_indices[i].item())

                    # Stop if all captions have reached the <eos> token
                    if all(end_token_appear):
                        break

                # Decode the generated captions from indices to text
            captions = [decoder(caption) for caption in captions]

        return captions
    def beam_search_caption(self, images, vocab, decoder, device="cpu",
                       start_token="<sos>", end_token="<eos>",
                       beam_width=3, max_seq_length=100):
        self.eval()

        with torch.no_grad():
            start_index = vocab[start_token]
            end_index = vocab[end_token]
            images = images.to(device)
            batch_size = images.size(0)

            # Ensure batch_size is 1 for beam search (one image at a time)
            if batch_size != 1:
                raise ValueError("Beam search currently supports batch_size=1.")

            cnn_features = self.cnn(images)  # (B, 49, 2048)
            h, c = self.lstm.init_hidden_state(batch_size)
            word_input = torch.full((batch_size,), start_index, dtype=torch.long).to(device)

            embeddings = self.lstm.embedding(word_input)
            context, _ = self.lstm.attention(cnn_features, h[-1])
            lstm_input = torch.cat([embeddings, context], dim=1).unsqueeze(1)


            sequences = [([start_index], 0.0, lstm_input, (h, c))]  # List of tuples: (sequence, score, input, state)

            completed_sequences = []

            for _ in range(max_seq_length):
                all_candidates = []

                for seq, score, lstm_input, (h,c) in sequences:
                    if seq[-1] == end_index:
                        completed_sequences.append((seq, score))
                        continue

                    lstm_out, (h_new, c_new) = model.lstm.lstm(lstm_input, (h, c))  # lstm_out: (1, 1, 1024)

                    output = model.lstm.fc(lstm_out.squeeze(1))  # Shape: (1, vocab_size)

                    log_probs = F.log_softmax(output, dim=1)  # Shape: (1, vocab_size)

                    top_log_probs, top_indices = log_probs.topk(beam_width, dim=1)  # Each of shape: (1, beam_width)

                    for i in range(beam_width):
                        token = top_indices[0, i].item()
                        token_log_prob = top_log_probs[0, i].item()

                        new_seq = seq + [token]
                        new_score = score + token_log_prob

                        token_tensor = torch.tensor([token], device=device)
                        embeddings = self.lstm.embedding(token_tensor)
                        context, _ = self.lstm.attention(cnn_features, h_new[-1])
                        new_lstm_input = torch.cat([embeddings, context], dim=1).unsqueeze(1)

                        if h_new is not None and c_new is not None:
                            h_new, c_new = (h_new.clone(), c_new.clone())
                        else:
                            h_new, c_new = None, None

                        all_candidates.append((new_seq, new_score, new_lstm_input, (h_new, c_new) ))

                if not all_candidates:
                    break

                ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)

                sequences = ordered[:beam_width]

                if len(completed_sequences) >= beam_width:
                    break

            if len(completed_sequences) == 0:
                completed_sequences = sequences

            best_seq = max(completed_sequences, key=lambda x: x[1])
            best_caption = decoder(best_seq[0])

        return best_caption

"""# ImgCap without attation"""

# class ResNet50(nn.Module):
#     def __init__(self):
#         super(ResNet50, self).__init__()
#         self.ResNet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

#         self.ResNet50.fc = nn.Sequential(
#                             nn.Linear(2048, 1024),
#                             nn.ReLU(),
#                             nn.Dropout(0.5),
#                             nn.Linear(1024, 1024),
#                             nn.ReLU(),
#                            )

#         for k,v in self.ResNet50.named_parameters(recurse=True):
#           if 'fc' in k:
#             v.requires_grad = True
#           else:
#             v.requires_grad = False

#     def forward(self,x):
#         return self.ResNet50(x)


# class lstm(nn.Module):
#     def __init__(self, input_size, hidden_size, number_layers, embedding_dim, vocab_size):
#         super(lstm, self).__init__()

#         self.input_size = input_size
#         self.hidden_size = hidden_size
#         self.number_layers = number_layers
#         self.embedding_dim = embedding_dim
#         self.vocab_size = vocab_size

#         self.embedding = nn.Embedding(vocab_size, hidden_size)
#         self.projection = nn.Linear(input_size, hidden_size)
#         self.relu = nn.ReLU()

#         self.lstm = nn.LSTM(
#             input_size=hidden_size,
#             hidden_size=hidden_size,
#             num_layers=number_layers,
# #             dropout=0.5,
#             batch_first=True,
#         )

#         self.fc = nn.Linear(hidden_size, vocab_size)

#     def forward(self, x, captions):
#         projected_image = self.projection(x).unsqueeze(dim=1)
#         embeddings = self.embedding(captions[:, :-1])

#         # Concatenate the image feature as frist step with word embeddings
#         lstm_input = torch.cat((projected_image, embeddings), dim=1)
#         # print(torch.all(projected_image[:, 0, :] == lstm_input[:, 0, :])) # check

#         lstm_out, _ = self.lstm(lstm_input)
#         logits = self.fc(lstm_out)

#         return logits



# class ImgCap(nn.Module):
#     def __init__(self, cnn_feature_size, lstm_hidden_size, num_layers, vocab_size, embedding_dim):
#         super(ImgCap, self).__init__()

#         self.cnn = ResNet50()

#         self.lstm = lstm(input_size=cnn_feature_size,
#                          hidden_size=lstm_hidden_size,
#                          number_layers=num_layers,
#                          embedding_dim=embedding_dim,
#                          vocab_size=vocab_size)

#     def forward(self, images, captions):
#         cnn_features = self.cnn(images)
#         output = self.lstm(cnn_features, captions)
#         return output

#     def generate_caption(self, images, vocab, decoder, device="cpu", start_token="<sos>", end_token="<eos>", max_seq_length=100):
#         self.eval()

#         with torch.no_grad():
#             start_index = vocab[start_token]
#             end_index = vocab[end_token]
#             images = images.to(device)
#             batch_size = images.size(0)

#             end_token_appear = {i: False for i in range(batch_size)}
#             captions = [[] for _ in range(batch_size)]

#             cnn_feature = self.cnn(images)
#             lstm_input = self.lstm.projection(cnn_feature).unsqueeze(1)  # (B, 1, hidden_size)

#             state = None

#             for i in range(max_seq_length):
#                 lstm_out, state = self.lstm.lstm(lstm_input, state)
#                 output = self.lstm.fc(lstm_out.squeeze(1))
#                 predicted_word_indices = torch.argmax(output, dim=1)
#                 lstm_input = self.lstm.embedding(predicted_word_indices).unsqueeze(1)  # (B, 1, hidden_size)

#                 for j in range(batch_size):
#                     if end_token_appear[j]:
#                         continue

#                     word = vocab.lookup_token(predicted_word_indices[j].item())
#                     if word == end_token:
#                         end_token_appear[j] = True

#                     captions[j].append(predicted_word_indices[j].item())

#             captions = [decoder(caption) for caption in captions]

#         return captions

"""### ImgCap"""

# Model Summary
model = ImgCap(feature_size=2048, lstm_hidden_size=1024, embedding_dim=1024, num_layers=2, vocab_size=len(vocab))
summary(model)

model(torch.randn(4,3,224,224), torch.randint(0, 1024, (4,64))).shape

image = torch.randn(2,3,224,224)
model.generate_caption(image, vocab, decoder)

image = torch.randn(1,3,224,224)
model.beam_search_caption(image, vocab, decoder)

# !pip install torcheval

"""## Eval Functions"""

from torcheval.metrics.functional.text import bleu_score
from pycocoevalcap.cider.cider import Cider

def eval_bleu_score(candidates, references, n_gram=4):
  '''
   args:
      candidates: list of strings (Generated caption)
      references: list of lists of strings (Ground truth)
      n_gram: int

    inputs example:
      candidates = ["this is a test", "this is another test"]
      references = [["this is a test"], ["this is another test"]] or ["this is a test", "this is another test"]

    returns:
      bleu score
  '''

  return bleu_score(input=candidates, target=references, n_gram=n_gram).item()


def eval_CIDEr(candidates, references, list_to_dict=True, inner_list=False):
  '''
   args:
      candidates: list of strings (Generated caption)
      references: list of lists of strings (Ground truth)
      list_to_dict: bool
      inner_list: bool

    inputs example:
      candidates = ["this is a test", "this is another test"]
      references = [["this is a test"], ["this is another test"]]

      or

      candidates = {0: "this is a test", 1: "this is another test"}
      references = {0: "this is a test", 1: "this is another test"}
      list_to_dict = False
      inner_list = True

      or

      candidates = {0: ["this is a test"], 1: ["this is another test"]}
      references = {0: ["this is a test"], 1: ["this is another test"]}
      list_to_dict = False
      inner_list = False

   return:
      avg_score: float
      scores: list of floats
  '''
  if list_to_dict:
    references = {i: [references[i]] for i in range(len(references))}
    candidates = {i: [candidates[i]] for i in range(len(candidates))}

  if inner_list:
    references = { k:[v] for k,v in references.items() }
    candidates = { k:[v] for k,v in candidates.items() }

  matric = Cider()
  avg_score , scores = matric.compute_score(res=references, gts=candidates)

  return avg_score, scores

def clean_caption(caption, vocab_decoder):
    words = []
    for token in caption:
        word = vocab_decoder([token])
        if word == '<eos>':
            words.append(token)
            break
        if word not in ['<pad>']:
            words.append(token)

    return vocab_decoder(words)



def eval_decode_batch(captions, vocab_decoder):
    deconed_captions = []

    for i in range(captions.shape[0]):

        ref_caption = clean_caption(captions[i, :].tolist(), vocab_decoder)
        deconed_captions.append(ref_caption)

    return deconed_captions

"""## Tranning Loop"""

torch.cuda.empty_cache()

import time
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import torch.optim.lr_scheduler as lr_scheduler
import torch.nn as nn
import numpy as np

def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):
    model.train()
    loss_running = []
    norm_avg = []

    for images, captions in train_loader:
        images, captions = images.to(device), captions.to(device)
        optimizer.zero_grad()

        with autocast(dtype=torch.float16):
            outputs = model(images, captions)
            try:
                loss = criterion(outputs.view(-1, outputs.size(2)), captions.contiguous().view(-1)) # outputs -> (B*S, vocab_size) , captions -> (B * S)
            except Exception as e:
                print(f"Error during training: {e}")
                print(f"Batch size: {captions.shape[0]}, Sequence length: {captions.shape[1]}")
                print(f"Outputs shape: {outputs.shape},  Captions shape: {captions.shape}")
                exit()

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
        norm_avg.append(norm.item())
        loss_running.append(loss.item())

    return np.mean(loss_running), np.mean(norm_avg)


def validate_model(model, val_loader, criterion, vocab, decoder, device):
    model.eval()
    val_loss = []
    bleu_scores = []
    cider_scores = []

    with torch.no_grad():
        for images, captions in val_loader:
            images, captions = images.to(device), captions.to(device)

            with autocast(dtype=torch.float16):
                outputs = model(images, captions)
                loss = criterion(outputs.view(-1, outputs.size(2)), captions.contiguous().view(-1))
                val_loss.append(loss.item())

                generated_captions = model.generate_caption(images, vocab, decoder, device)
                decoded_captions = eval_decode_batch(captions, decoder)

                bleu4_score = eval_bleu_score(candidates=generated_captions, references=decoded_captions)
                cider_score, _ = eval_CIDEr(candidates=generated_captions, references=decoded_captions)
                bleu_scores.append(bleu4_score)
                cider_scores.append(cider_score)

    return np.mean(val_loss), np.mean(bleu_scores), np.mean(cider_scores), generated_captions, decoded_captions


def train_model(model, train_loader, val_loader, criterion, optimizer, scaler, num_epochs, device, vocab, decoder):
    for epoch in range(num_epochs):
        start_time = time.time()

        train_loss, norm = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)
        epoch_time = time.time() - start_time

        if (epoch + 1) % 5 == 0:
            val_loss, avg_bleu, avg_cider, generated_captions, decoded_captions = validate_model(
                model, val_loader, criterion, vocab, decoder, device
            )

            epoch_time = time.time() - start_time

            print(f"Epoch [{epoch + 1}/{num_epochs}], "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"BLEU Score: {avg_bleu:.4f}, "
                  f"CIDEr Score: {avg_cider:.4f}, "
                  f"Norm: {norm:.2f}, "
                  f"Epoch Time: {epoch_time:.2f} sec")
            print("-" * 120)
            print(f"Generated Caption Example: {generated_captions[0]}")
            print(f"Ground Truth Caption Example: {decoded_captions[0]}")
            print("-" * 120)
        else:
            print(f"Epoch [{epoch + 1}/{num_epochs}], "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Norm: {norm:.2f}, "
                  f"Epoch Time: {epoch_time:.2f} sec")

    return model, optimizer, train_loss, val_loss

decoder = dataset.decoder
vocab = dataset.vocab

criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = GradScaler()

model = ImgCap(feature_size=2048, lstm_hidden_size=1024, embedding_dim=1024, num_layers=2, vocab_size=len(vocab))
optimizer = optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-4)
model.to(device)

# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

num_epochs = 50

model, optimizer, train_loss, val_loss = train_model(
    model, train_data_loader, val_data_loader, criterion, optimizer, scaler, num_epochs, device, vocab, decoder
)

"""### check"""

import time
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import torch.nn as nn
import numpy as np

decoder = dataset.decoder
vocab = dataset.vocab

criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model =  ImgCap(feature_size=2048, lstm_hidden_size=512, embedding_dim=1024, num_layers=2, vocab_size=len(vocab))
optimizer = optim.AdamW(model.parameters(), lr=0.004)
model.to(device)

fixed_batch = next(iter(train_data_loader))
images, captions = fixed_batch
images, captions = images.to(device), captions.to(device)

for epoch in range(250):
    model.train()
    optimizer.zero_grad()

    outputs = model(images, captions)
    loss = criterion(outputs.view(-1, outputs.size(2)), captions.view(-1))
    loss.backward()
    optimizer.step()

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

    model.eval()
    with torch.no_grad():
        img, cap = images[0], captions[0]
        img = img.unsqueeze(0)

        gencab = model.generate_caption(img.to(device), vocab, decoder, device='cuda')


        print("Generated Caption: "+gencab[0])
        print("Ground Truth Caption: " + dataset.decoder(cap))

    if loss.item() < 0.01:
        print("Overfitting achieved, stopping early!")
        break

gencap = model.generate_caption(images, vocab, decoder, 'cuda')

for i in range(16):
    print(f"ground truth{dataset.decoder(captions[i])}")
    print(f"genrited capution {gencap[i]}")
    print()

"""## Play"""

# def generate_caption(model, images, vocab, decode, device="cpu", start_token="<sos>", end_token="<eos>", max_seq_length=100):
#     model.eval()

#     with torch.no_grad():
#         start_index = vocab[start_token]
#         end_index = vocab[end_token]
#         images = images.to(device)
#         batch_size = images.size(0)

#         end_token_appear = {i: False for i in range(batch_size)}
#         captions = [[] for _ in range(batch_size)]

#         cnn_features = model.cnn(images)  # (B, 49, 2048)
#         state = model.lstm.init_hidden_state(batch_size)

#         word = torch.tensor([start_index] * batch_size).to(device)  # Start token

#         for i in range(max_seq_length):
#             embeddings = model.lstm.embedding(word)  # (B, hidden_size)
#             context, _ = model.lstm.attention(cnn_features, state[0][-1])  # Attention context
#             lstm_input = torch.cat([embeddings, context], dim=1).unsqueeze(1)  # (B, 1, hidden_size + feature_size)

#             lstm_out, state = model.lstm.lstm(lstm_input, state)  # (B, 1, hidden_size)
#             output = model.lstm.fc(lstm_out.squeeze(1))  # (B, vocab_size)
#             predicted_word_indices = torch.argmax(output, dim=1)  # (B,)
#             print(predicted_word_indices)
#             for j in range(batch_size):
#                 if end_token_appear[j]:
#                     continue
#                 word = vocab.lookup_token(predicted_word_indices[j].item())
#                 if word == end_token:
#                     end_token_appear[j] = True
#                 captions[j].append(predicted_word_indices[j].item())

#             word = predicted_word_indices  # Use predicted word as next input

#         captions = [decode(caption) for caption in captions]

#     return captions

import matplotlib.pyplot as plt
idx = torch.randint(0, 1024, (9,))
images, ground_truth_captions = [], []

for i in idx:
    image, ground_truth_caption = train_data_loader.dataset[i]
    images.append(image)
    ground_truth_captions.append(ground_truth_caption)

images = torch.stack(images).to("cuda")
model.to("cuda")
model.eval()
generated_captions = model.generate_caption(images, vocab, decoder, device="cuda")

for i in range(len(images)):
    print()
    print(f"Ground Truth Caption: {dataset.decoder(ground_truth_captions[i])}")
    print(f"Generated Caption: {generated_captions[i]}")
    print()
    plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())
    plt.show()

"""## Beam Search"""

model.eval()
import matplotlib.pyplot as plt
l = 5
idx = torch.randint(0, 1024, (l,))
images, ground_truth_captions, generated_captions = [], [] , []

for i in idx:
    image, ground_truth_caption = train_data_loader.dataset[i]
    img = torch.tensor(image).unsqueeze(0).clone().detach()
    generated_caption = model.beam_search_caption(img, vocab, decoder, device="cuda")
    generated_captions.append(generated_caption)
    ground_truth_captions.append(ground_truth_caption)
    images.append(image)


for i in range(l):
    print()
    print(f"Ground Truth Caption: {dataset.decoder(ground_truth_captions[i])}")
    print(f"Generated Caption: {generated_captions[i]}")
    print()
    plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())
    plt.show()

"""## Analsis"""

# ToDo:

# More Data
# Try More Batch Size
# Increase the Vocab size
# Pading affect of less Issue  (fixed)
# Use Torch Compile
# Try Less hidding uint for Lstm (done)
# add other Lstm Cell
# Implement a victorised a way for Lstm (by stacking the image with word embeding)

# Note:
# embedding_dim must equal CNN Feature Size
