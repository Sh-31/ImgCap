# -*- coding: utf-8 -*-
"""imgcapattation | prototypinga.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2DnEOtghSUk1JHEIqMGGMXHmNV_0CF4

### Imports
"""

# pip install -q  torch==2.2.0 opencv-python torchvision==0.17 torchtext==0.17.0 pycocoevalcap onnx torchinfo datasets
# python -q -m spacy download en_core_web_sm
# wget https://raw.githubusercontent.com/Sh-31/ImgCap/main/vocab.pkl
# pip install -q torcheval

"""### DataLoader"""

import torch
import torch.nn as nn
import numpy as np
import torch.onnx as onnx
import torchvision.models as models
import torch.nn.functional as F
import pandas as pd
import pickle
import string
import spacy
import cv2
from torch.utils.data import Dataset , random_split
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
from torchinfo import summary
from datasets import load_dataset
from torchvision.transforms import v2
from torch.nn.utils.rnn import pad_sequence

ds = load_dataset("shredder-31/Flickr30_sample")

ds

class Flickr30(Dataset):
    def __init__(self, dataset, vocab=None, vocab_path='ImgCap/vocab.pkl',transform=None):
        self.dataset = dataset
        self.transform = transform

        # Initialize spaCy model for tokenization
        self.nlp = spacy.load('en_core_web_sm')  # Can use split(' ')

        if vocab is None:
            self.vocab = self.build_vocab()
            self.save_vocab(self.vocab, vocab_path)
        else:
            self.vocab = vocab

    def build_vocab(self):
        '''Tokenize all captions and build vocabulary'''

        captions = self.dataset['train']['comment']
        tokenized_captions = [self.tokenize_caption(caption) for caption in captions]

        # 997 (token) + 26 (English alphabet) + space = 1024 tokens
        vocab = build_vocab_from_iterator(tokenized_captions, specials=["<unk>", "<pad>", "<sos>", "<eos>"], max_tokens=997, min_freq=5)

        # Add individual English alphabet characters and space to the vocabulary
        for ch in string.ascii_lowercase + ' ':
            if ch not in vocab:
                vocab.append_token(ch)

        vocab.set_default_index(vocab["<unk>"])
        return vocab

    def save_vocab(self, vocab, vocab_path):
        with open(vocab_path, 'wb') as f:
            pickle.dump(vocab, f)
        print(f"Vocabulary saved at {vocab_path}")

    def tokenize_caption(self, caption):
        return [f" {token.text.lower()}" for token in self.nlp(caption)]

    def encoder(self, text):
        tokens = self.tokenize_caption(text)
        indices = [self.vocab["<sos>"]]

        for token in tokens:
            if token in self.vocab:
                indices.append(self.vocab[token])
            else:
                # Split the word into characters if the word is not found in the vocabulary
                for ch in token:
                    indices.append(self.vocab[ch])

        indices.append(self.vocab["<eos>"])
        return torch.tensor(indices, dtype=torch.long)

    def decoder(self, indices):
        tokens = [self.vocab.lookup_token(idx) for idx in indices]
        words = []
        current_word = []
        for token in tokens:
            if len(token) == 1 and token in string.ascii_lowercase:
                current_word.append(token)
            else:
                if current_word:
                    words.append("".join(current_word))
                    current_word = []
                words.append(token)

        if current_word:
            words.append(" "+"".join(current_word))

        return "".join(words)

    def __len__(self):
        return len(self.dataset['train'])

    def __getitem__(self, idx):
        sample = self.dataset['train'][idx]
        image = sample['image']
        caption = sample['comment']

        if self.transform:
            image = self.transform(image)

        tensor_caption = self.encoder(caption)

        return image, tensor_caption

transforms = v2.Compose([
    v2.Resize(size=(224, 224)),
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

with open("/content/vocab.pkl", 'rb') as f:
      vocab = pickle.load(f)

dataset = Flickr30(ds,vocab=vocab, transform=transforms)

def collate_fn(batch):
    images, captions = zip(*batch)
    images = torch.stack(images, 0)
    captions = pad_sequence(captions, batch_first=True, padding_value=1)
    return images, captions

train_size = int(0.80 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)
val_data_loader = DataLoader(val_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)

for image, cap in train_data_loader:
  print(dataset.decoder(cap[7]))
  break

decoder = dataset.decoder
vocab = dataset.vocab

"""### Modeling"""

## ResNet50 (CNN Encoder) with Spatial Features

class ResNet50(nn.Module):
    def __init__(self):
        super(ResNet50, self).__init__()
        self.ResNet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

        self.features = nn.Sequential(*list(self.ResNet50.children())[:-2])
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))

        for param in self.ResNet50.parameters():
            param.requires_grad = False

    def forward(self, x):
        x = self.features(x)  # (B, 2048, 7, 7)
        x = self.avgpool(x)
        B, C, H, W = x.size()
        x = x.view(B, C, -1)    # Flatten spatial dimensions: (B, 2048, 49)
        x = x.permute(0, 2, 1)  # (B, 49, 2048) - 49 spatial locations
        return x

# x = torch.randint(0, 255, (4, 3, 224, 224)).float()  # Convert to float
# x = x / 255.0
# encoder = ResNet50()
# out = encoder(x)
# print(out.shape)

class Attention(nn.Module):
    def __init__(self, feature_size, hidden_size):
        super(Attention, self).__init__()
        self.attention = nn.Linear(feature_size + hidden_size, hidden_size)
        self.attn_weights = nn.Linear(hidden_size, 1)

    def forward(self, features, hidden_state): # features: (B, 49, 2048), hidden_state: (B, hidden_size)

        hidden_state = hidden_state.unsqueeze(1).repeat(1, features.size(1), 1)  # (B, 49, hidden_size)
        combined = torch.cat((features, hidden_state), dim=2)  # (B, 49, feature_size + hidden_size)

        attn_hidden = torch.tanh(self.attention(combined))  # (B, 49, hidden_size)
        attention_logits = self.attn_weights(attn_hidden).squeeze(2)  # (B, 49)
        attention_weights = torch.softmax(attention_logits, dim=1)  # (B, 49)
        context = (features * attention_weights.unsqueeze(2)).sum(dim=1)  # (B, 2048)
        return context, attention_weights

# Attention without learnable paramters:
# logits = torch.matmul(features, hidden_state.unsqueeze(2))  # (B, 49, 1) - Batch Matriax
# attention_weights = torch.softmax(logits, dim=1).squeeze(2)  # (B, 49)
# context = (features * attention_weights.unsqueeze(2)).sum(dim=1)  # (B, 2048)

class lstm(nn.Module):
    def __init__(self, feature_size, hidden_size, number_layers, embedding_dim, vocab_size):
        super(lstm, self).__init__()

        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.attention = Attention(feature_size, hidden_size)

        self.lstm = nn.LSTM(
            input_size=hidden_size + feature_size,  # input: concatenated context and word embedding
            hidden_size=hidden_size,
            num_layers=number_layers,
            dropout=0.5,
            batch_first=True,
        )

        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions):
        embeddings = self.embedding(captions)  # (B, max_len-1, hidden_size)
        h, c = self.init_hidden_state(features.size(0))

        logits = torch.zeros(embeddings.size(0), embeddings.size(1), self.fc.out_features).to(embeddings.device)  # logits: B, max_len-1, 4048
        for t in range(embeddings.size(1)):
            context, _ = self.attention(features, h[-1])
            lstm_input_step = torch.cat([embeddings[:, t, :], context], dim=1).unsqueeze(1)  # Combine context + word embedding

            out, (h, c) = self.lstm(lstm_input_step, (h, c))
            logits[:, t, :] = self.fc(out.squeeze(1))  # (B, 4048)

        return logits

    def init_hidden_state(self, batch_size):
        device = next(self.parameters()).device
        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_size).to(device)
        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_size).to(device)
        return (h0, c0)

## ImgCap model with Attention
class ImgCap(nn.Module):
    def __init__(self, feature_size, lstm_hidden_size, num_layers, vocab_size, embedding_dim):
        super(ImgCap, self).__init__()

        self.cnn = ResNet50()

        self.lstm = lstm(feature_size=feature_size,
                         hidden_size=lstm_hidden_size,
                         number_layers=num_layers,
                         embedding_dim=embedding_dim,
                         vocab_size=vocab_size)

    def forward(self, images, captions):
        cnn_features = self.cnn(images)  # (B, 49, 2048)
        output = self.lstm(cnn_features, captions)
        return output

    def generate_caption(self, images, vocab, decoder, device="cpu", start_token="<sos>", end_token="<eos>", max_seq_length=100):
        self.eval()

        with torch.no_grad():
            start_index = vocab[start_token]
            end_index = vocab[end_token]
            images = images.to(device)
            batch_size = images.size(0)

            end_token_appear = {i: False for i in range(batch_size)}
            captions = [[] for _ in range(batch_size)]

            cnn_features = self.cnn(images)  # (B, 49, 2048)
            h, c = self.lstm.init_hidden_state(batch_size)

            word_idx = torch.tensor([start_index] * batch_size).to(device)  # Start token

            for i in range(max_seq_length):
                embeddings = self.lstm.embedding(word_idx)  # (B, hidden_size)
                context, _ = self.lstm.attention(cnn_features, h[-1])  # Attention context
                lstm_input = torch.cat([embeddings, context], dim=1).unsqueeze(1)  # (B, 1, hidden_size + feature_size)

                lstm_out, (h, c) = self.lstm.lstm(lstm_input, (h, c))  # (B, 1, hidden_size)
                output = self.lstm.fc(lstm_out.squeeze(1))  # (B, vocab_size)
                predicted_word_indices = torch.argmax(output, dim=1)  # (B,)
                word_idx = predicted_word_indices  # Use predicted word as next input

                for j in range(batch_size):
                    if end_token_appear[j]:
                        continue
                    word = vocab.lookup_token(predicted_word_indices[j].item())
                    if word == end_token:
                        end_token_appear[j] = True
                    captions[j].append(predicted_word_indices[j].item())



            captions = [decoder(caption) for caption in captions]

        return captions



"""# ImgCap"""

# Model Summary
model = ImgCap(feature_size=2048, lstm_hidden_size=1024, embedding_dim=1024, num_layers=2, vocab_size=len(vocab))
summary(model)

model(torch.randn(4,3,224,224), torch.randint(0, 1024, (4,60))).shape

!pip install torcheval

"""# Eval Functions"""

from torcheval.metrics.functional.text import bleu_score
from pycocoevalcap.cider.cider import Cider

def eval_bleu_score(candidates, references, n_gram=4):
  '''
   args:
      candidates: list of strings (Generated caption)
      references: list of lists of strings (Ground truth)
      n_gram: int

    inputs example:
      candidates = ["this is a test", "this is another test"]
      references = [["this is a test"], ["this is another test"]] or ["this is a test", "this is another test"]

    returns:
      bleu score
  '''

  return bleu_score(input=candidates, target=references, n_gram=n_gram).item()


def eval_CIDEr(candidates, references, list_to_dict=True, inner_list=False):
  '''
   args:
      candidates: list of strings (Generated caption)
      references: list of lists of strings (Ground truth)
      list_to_dict: bool
      inner_list: bool

    inputs example:
      candidates = ["this is a test", "this is another test"]
      references = [["this is a test"], ["this is another test"]]

      or

      candidates = {0: "this is a test", 1: "this is another test"}
      references = {0: "this is a test", 1: "this is another test"}
      list_to_dict = False
      inner_list = True

      or

      candidates = {0: ["this is a test"], 1: ["this is another test"]}
      references = {0: ["this is a test"], 1: ["this is another test"]}
      list_to_dict = False
      inner_list = False

   return:
      avg_score: float
      scores: list of floats
  '''
  if list_to_dict:
    references = {i: [references[i]] for i in range(len(references))}
    candidates = {i: [candidates[i]] for i in range(len(candidates))}

  if inner_list:
    references = { k:[v] for k,v in references.items() }
    candidates = { k:[v] for k,v in candidates.items() }

  matric = Cider()
  avg_score , scores = matric.compute_score(res=references, gts=candidates)

  return avg_score, scores

def clean_caption(caption, vocab_decoder):
    words = []
    for token in caption:
        word = vocab_decoder([token])
        if word == '<eos>':
            words.append(token)
            break
        if word not in ['<pad>']:
            words.append(token)

    return vocab_decoder(words)



def eval_decode_batch(captions, vocab_decoder):
    deconed_captions = []

    for i in range(captions.shape[0]):

        ref_caption = clean_caption(captions[i, :].tolist(), vocab_decoder)
        deconed_captions.append(ref_caption)

    return deconed_captions

"""# Tranning Loop"""

torch.cuda.empty_cache()

import time
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import torch.optim.lr_scheduler as lr_scheduler
import torch.nn as nn
import numpy as np

def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):
    model.train()
    loss_running = []
    norm_avg = []

    for images, captions in train_loader:
        images, captions = images.to(device), captions.to(device)
        optimizer.zero_grad()

        with autocast(dtype=torch.float16):
            outputs = model(images, captions)
            try:
                loss = criterion(outputs.view(-1, outputs.size(2)), captions.contiguous().view(-1)) # outputs -> (B*S, vocab_size) , captions -> (B * S)
            except Exception as e:
                print(f"Error during training: {e}")
                print(f"Batch size: {captions.shape[0]}, Sequence length: {captions.shape[1]}")
                print(f"Outputs shape: {outputs.shape},  Captions shape: {captions.shape}")
                exit()

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
        norm_avg.append(norm.item())
        loss_running.append(loss.item())

    return np.mean(loss_running), np.mean(norm_avg)


def validate_model(model, val_loader, criterion, vocab, decoder, device):
    model.eval()
    val_loss = []
    bleu_scores = []
    cider_scores = []

    with torch.no_grad():
        for images, captions in val_loader:
            images, captions = images.to(device), captions.to(device)

            with autocast(dtype=torch.float16):
                outputs = model(images, captions)
                loss = criterion(outputs.view(-1, outputs.size(2)), captions.contiguous().view(-1))
                val_loss.append(loss.item())

                generated_captions = model.generate_caption(images, vocab, decoder, device)
                decoded_captions = eval_decode_batch(captions, decoder)

                bleu4_score = eval_bleu_score(candidates=generated_captions, references=decoded_captions)
                cider_score, _ = eval_CIDEr(candidates=generated_captions, references=decoded_captions)
                bleu_scores.append(bleu4_score)
                cider_scores.append(cider_score)

    return np.mean(val_loss), np.mean(bleu_scores), np.mean(cider_scores), generated_captions, decoded_captions


def train_model(model, train_loader, val_loader, criterion, optimizer, scaler, scheduler, num_epochs, device, vocab, decoder):
    for epoch in range(num_epochs):
        start_time = time.time()

        train_loss, norm = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)
        scheduler.step()
        epoch_time = time.time() - start_time

        if (epoch + 1) % 10 == 0:
            val_loss, avg_bleu, avg_cider, generated_captions, decoded_captions = validate_model(
                model, val_loader, criterion, vocab, decoder, device
            )

            epoch_time = time.time() - start_time

            print(f"Epoch [{epoch + 1}/{num_epochs}], "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"BLEU Score: {avg_bleu:.4f}, "
                  f"CIDEr Score: {avg_cider:.4f}, "
                  f"Norm: {norm:.2f}, "
                  f"Epoch Time: {epoch_time:.2f} sec")
            print("-" * 120)
            print(f"Generated Caption Example: {generated_captions[0]}")
            print(f"Ground Truth Caption Example: {decoded_captions[0]}")
            print("-" * 120)
        else:
            print(f"Epoch [{epoch + 1}/{num_epochs}], "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Norm: {norm:.2f}, "
                  f"Epoch Time: {epoch_time:.2f} sec")

    return model, optimizer, train_loss, val_loss

decoder = dataset.decoder
vocab = dataset.vocab

criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = GradScaler()

model = ImgCap(feature_size=2048, lstm_hidden_size=1024, embedding_dim=1024, num_layers=2, vocab_size=len(vocab))
optimizer = optim.AdamW(model.parameters(), lr=4e-4, weight_decay=1e-4)
model.to(device)

scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)



num_epochs = 50

model, optimizer, train_loss, val_loss = train_model(
    model, train_data_loader, val_data_loader, criterion, optimizer, scaler, scheduler, num_epochs, device, vocab, decoder
)

# checkpoint = {
#     'epoch': num_epochs,.
#     'model_state_dict': model.state_dict(),
#     'optimizer_state_dict': optimizer.state_dict(),
#     'scaler_state_dict': scaler.state_dict(),
#     'train_loss': train_loss,
#     'val_loss': val_loss
# }

# torch.save(checkpoint, 'checkpoint.pth')

"""### check"""

import time
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import torch.nn as nn
import numpy as np

decoder = dataset.decoder
vocab = dataset.vocab

criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = GradScaler()

model =  ImgCap(feature_size=2048, lstm_hidden_size=512, embedding_dim=512, num_layers=2, vocab_size=len(vocab))
optimizer = optim.AdamW(model.parameters(), lr=4e-4)
model.to(device)

fixed_batch = next(iter(train_data_loader))
images, captions = fixed_batch
images, captions = images.to(device), captions.to(device)

print()

optimizer = optim.AdamW(model.parameters(), lr=0.01)

for epoch in range(250):
    model.train()
    optimizer.zero_grad()

    outputs = model(images, captions)
    loss = criterion(outputs.view(-1, outputs.size(2)), captions.view(-1))
    loss.backward()
    optimizer.step()

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

    model.eval()
    with torch.no_grad():
        img, cap = images[0], captions[0]
        img = img.unsqueeze(0).to(device) # add batch dim
        B = img.size(0)

        cnn_features = model.cnn(img)
        h , c = model.lstm.init_hidden_state(B)
        word = torch.tensor(vocab["<sos>"]).unsqueeze(0).to(device)

        words = []
        seq_gen = 35
        for _ in range(seq_gen):
            embeddings = model.lstm.embedding(word) # (B, hidden_size)
            context, _ = model.lstm.attention(cnn_features, h[-1])  # Attention context

            lstm_input = torch.cat([embeddings, context], dim=1)  # (B, 1, hidden_size + feature_size)
            lstm_input = lstm_input.unsqueeze(1)
            lstm_out, (h , c) = model.lstm.lstm(lstm_input, (h , c))

            output = model.lstm.fc(lstm_out.squeeze(1))  # (B, vocab_size)
            predicted_word_indices = torch.argmax(output, dim=1)  # (B,)
            word = predicted_word_indices
            words.append(word.item())

        print("Generated Caption: " + dataset.decoder(words))
        print("Ground Truth Caption: " + dataset.decoder(cap))

    if loss.item() < 0.01:
        print("Overfitting achieved, stopping early!")
        break

gencap = model.generate_caption(images, vocab, decoder, 'cuda')

for i in range(16):
    print(f"ground truth{dataset.decoder(captions[i])}")
    print(f"genrited capution {gencap[i]}")
    print()

"""## Play"""

def generate_caption(model, images, vocab, decode, device="cpu", start_token="<sos>", end_token="<eos>", max_seq_length=100):
    model.eval()

    with torch.no_grad():
        start_index = vocab[start_token]
        end_index = vocab[end_token]
        images = images.to(device)
        batch_size = images.size(0)

        end_token_appear = {i: False for i in range(batch_size)}
        captions = [[] for _ in range(batch_size)]

        cnn_features = model.cnn(images)  # (B, 49, 2048)
        state = model.lstm.init_hidden_state(batch_size)

        word = torch.tensor([start_index] * batch_size).to(device)  # Start token

        for i in range(max_seq_length):
            embeddings = model.lstm.embedding(word)  # (B, hidden_size)
            context, _ = model.lstm.attention(cnn_features, state[0][-1])  # Attention context
            lstm_input = torch.cat([embeddings, context], dim=1).unsqueeze(1)  # (B, 1, hidden_size + feature_size)

            lstm_out, state = model.lstm.lstm(lstm_input, state)  # (B, 1, hidden_size)
            output = model.lstm.fc(lstm_out.squeeze(1))  # (B, vocab_size)
            predicted_word_indices = torch.argmax(output, dim=1)  # (B,)
            print(predicted_word_indices)
            for j in range(batch_size):
                if end_token_appear[j]:
                    continue
                word = vocab.lookup_token(predicted_word_indices[j].item())
                if word == end_token:
                    end_token_appear[j] = True
                captions[j].append(predicted_word_indices[j].item())

            word = predicted_word_indices  # Use predicted word as next input

        captions = [decode(caption) for caption in captions]

    return captions

import matplotlib.pyplot as plt
idx = torch.randint(0, 1024, (2,))
images, ground_truth_captions = [], []

for i in idx:
    image, ground_truth_caption = train_data_loader.dataset[i]
    images.append(image)
    ground_truth_captions.append(ground_truth_caption)

images = torch.stack(images).to("cuda")
model.to("cuda")
model.eval()
generated_captions = generate_caption(model, images, vocab, decoder, device="cuda")

for i in range(len(images)):
    print()
    print(f"Ground Truth Caption: {dataset.decoder(ground_truth_captions[i])}")
    print(f"Generated Caption: {generated_captions[i]}")
    print()
    plt.imshow(images[i].permute(1, 2, 0).cpu().numpy())
    plt.show()

